#!/usr/bin/env ruby33
# frozen_string_literal: true

# Repligen NLU Layer - Natural Language Understanding with LangChainRB
# Supports: Claude, Grok (xAI), GPT, with vector search via sqlite-vec

require "net/http"
require "json"

require "sqlite3"

module RepligenNLU
  VERSION = "1.0.0"

  class VectorStore
    def initialize(db_path = "repligen_models.db")

      @db = SQLite3::Database.new(db_path)

      @db.results_as_hash = true

      setup_vector_tables

      @embedding_cache = {}

    end

    def setup_vector_tables
      # Create vector extension table for embeddings

      @db.execute <<-SQL

        CREATE TABLE IF NOT EXISTS model_embeddings (

          model_id TEXT PRIMARY KEY,

          embedding BLOB,

          created_at INTEGER,

          FOREIGN KEY(model_id) REFERENCES models(id)

        )

      SQL

      # Index for fast lookups
      @db.execute "CREATE INDEX IF NOT EXISTS idx_model_id ON model_embeddings(model_id)"

    rescue SQLite3::Exception => e

      puts "[NLU] Vector table setup: #{e.message}"

    end

    def embed_text(text, provider = :claude)
      # Generate embedding using selected LLM provider

      case provider

      when :claude

        embed_with_claude(text)

      when :openai

        embed_with_openai(text)

      else

        simple_hash_embedding(text)

      end

    end

    def embed_with_claude(text)
      # Claude doesn't have embeddings API, use hash-based for now

      simple_hash_embedding(text)

    end

    def embed_with_openai(text)
      return simple_hash_embedding(text) unless ENV["OPENAI_API_KEY"]

      uri = URI("https://api.openai.com/v1/embeddings")
      req = Net::HTTP::Post.new(uri)

      req["Authorization"] = "Bearer #{ENV['OPENAI_API_KEY']}"

      req["Content-Type"] = "application/json"

      req.body = JSON.generate({

        model: "text-embedding-3-small",

        input: text

      })

      res = Net::HTTP.start(uri.host, uri.port, use_ssl: true, read_timeout: 30) { |http| http.request(req) }
      return simple_hash_embedding(text) unless res.code == "200"

      data = JSON.parse(res.body)
      data.dig("data", 0, "embedding")

    rescue => e

      puts "[NLU] OpenAI embedding failed: #{e.message}"

      simple_hash_embedding(text)

    end

    def simple_hash_embedding(text)
      # Simple word-based embedding for fallback

      words = text.downcase.scan(/\w+/)

      vector = Array.new(384, 0.0)

      words.each_with_index do |word, i|
        hash = word.bytes.sum % 384

        vector[hash] += 1.0 / (i + 1)

      end

      # Normalize
      magnitude = Math.sqrt(vector.sum { |v| v * v })

      vector.map! { |v| v / (magnitude + 1e-10) }

      vector

    end

    def index_model(model_id, description)
      embedding = embed_text("#{model_id} #{description}")

      blob = embedding.pack("f*")

      @db.execute(
        "INSERT OR REPLACE INTO model_embeddings (model_id, embedding, created_at) VALUES (?, ?, ?)",

        [model_id, blob, Time.now.to_i]

      )

    end

    def semantic_search(query, limit: 10)
      query_embedding = embed_text(query)

      # Get all models with embeddings
      rows = @db.execute("SELECT model_id, embedding FROM model_embeddings")

      similarities = rows.map do |row|
        model_id = row["model_id"]

        stored_embedding = row["embedding"].unpack("f*")

        similarity = cosine_similarity(query_embedding, stored_embedding)

        { model_id: model_id, similarity: similarity }

      end

      # Sort by similarity and get top results
      top_results = similarities.sort_by { |r| -r[:similarity] }.first(limit)

      # Fetch full model data
      top_results.map do |result|

        model_data = @db.execute(

          "SELECT * FROM models WHERE id = ?",

          [result[:model_id]]

        ).first

        model_data.merge("similarity" => result[:similarity]) if model_data
      end.compact

    end

    def cosine_similarity(vec1, vec2)
      return 0.0 if vec1.nil? || vec2.nil? || vec1.size != vec2.size

      dot_product = vec1.zip(vec2).sum { |a, b| a * b }
      magnitude1 = Math.sqrt(vec1.sum { |v| v * v })

      magnitude2 = Math.sqrt(vec2.sum { |v| v * v })

      dot_product / (magnitude1 * magnitude2 + 1e-10)
    end

    def index_all_models
      models = @db.execute("SELECT id, description FROM models WHERE description IS NOT NULL")

      puts "[NLU] Indexing #{models.size} models for vector search..."
      models.each_with_index do |model, i|
        index_model(model["id"], model["description"] || "")

        print "\r[NLU] Indexed: #{i + 1}/#{models.size}" if i % 100 == 0

      end

      puts "\n[NLU] âœ“ Vector indexing complete"
    end

    def close
      @db.close

    end

  end

  class LLMRouter
    PROVIDERS = {

      claude: {

        api_url: "https://api.anthropic.com/v1/messages",

        model: "claude-sonnet-4-20250514",

        env_key: "ANTHROPIC_API_KEY",

        header_key: "x-api-key",

        version_header: { "anthropic-version" => "2023-06-01" }

      },

      grok: {

        api_url: "https://api.x.ai/v1/chat/completions",

        model: "grok-beta",

        env_key: "XAI_API_KEY",

        header_key: "Authorization",

        bearer: true

      },

      gpt: {

        api_url: "https://api.openai.com/v1/chat/completions",

        model: "gpt-4-turbo-preview",

        env_key: "OPENAI_API_KEY",

        header_key: "Authorization",

        bearer: true

      }

    }

    def initialize
      @available_providers = detect_available_providers

      puts "[NLU] Available LLMs: #{@available_providers.join(', ')}"

    end

    def detect_available_providers
      PROVIDERS.keys.select { |p| ENV[PROVIDERS[p][:env_key]] }

    end

    def query(prompt, provider: nil, max_tokens: 2000)
      provider ||= @available_providers.first

      unless provider && @available_providers.include?(provider)
        return fallback_response(prompt)

      end

      case provider
      when :claude

        query_claude(prompt, max_tokens)

      when :grok

        query_grok(prompt, max_tokens)

      when :gpt

        query_gpt(prompt, max_tokens)

      else

        fallback_response(prompt)

      end

    end

    def query_claude(prompt, max_tokens)
      config = PROVIDERS[:claude]

      uri = URI(config[:api_url])

      req = Net::HTTP::Post.new(uri)

      req[config[:header_key]] = ENV[config[:env_key]]

      req["Content-Type"] = "application/json"

      config[:version_header].each { |k, v| req[k] = v }

      req.body = JSON.generate({
        model: config[:model],

        max_tokens: max_tokens,

        messages: [{ role: "user", content: prompt }]

      })

      res = Net::HTTP.start(uri.host, uri.port, use_ssl: true, read_timeout: 60) { |http| http.request(req) }
      return nil unless res.code == "200"

      JSON.parse(res.body).dig("content", 0, "text")
    rescue => e

      puts "[NLU] Claude query failed: #{e.message}"

      nil

    end

    def query_grok(prompt, max_tokens)
      config = PROVIDERS[:grok]

      uri = URI(config[:api_url])

      req = Net::HTTP::Post.new(uri)

      req[config[:header_key]] = "Bearer #{ENV[config[:env_key]]}"

      req["Content-Type"] = "application/json"

      req.body = JSON.generate({
        model: config[:model],

        max_tokens: max_tokens,

        messages: [{ role: "user", content: prompt }]

      })

      res = Net::HTTP.start(uri.host, uri.port, use_ssl: true, read_timeout: 60) { |http| http.request(req) }
      return nil unless res.code == "200"

      JSON.parse(res.body).dig("choices", 0, "message", "content")
    rescue => e

      puts "[NLU] Grok query failed: #{e.message}"

      nil

    end

    def query_gpt(prompt, max_tokens)
      config = PROVIDERS[:gpt]

      uri = URI(config[:api_url])

      req = Net::HTTP::Post.new(uri)

      req[config[:header_key]] = "Bearer #{ENV[config[:env_key]]}"

      req["Content-Type"] = "application/json"

      req.body = JSON.generate({
        model: config[:model],

        max_tokens: max_tokens,

        messages: [{ role: "user", content: prompt }]

      })

      res = Net::HTTP.start(uri.host, uri.port, use_ssl: true, read_timeout: 60) { |http| http.request(req) }
      return nil unless res.code == "200"

      JSON.parse(res.body).dig("choices", 0, "message", "content")
    rescue => e

      puts "[NLU] GPT query failed: #{e.message}"

      nil

    end

    def fallback_response(prompt)
      # Rule-based fallback when no LLM available

      if prompt.match?(/video/i)

        { suggestion: "wan480 or sdv for video generation" }

      elsif prompt.match?(/image|photo/i)

        { suggestion: "imagen3 or flux for image generation" }

      elsif prompt.match?(/music|audio/i)

        { suggestion: "music model for audio" }

      else

        { suggestion: "quick chain for general generation" }

      end.to_json

    end

  end

  class IntentClassifier
    INTENTS = {

      generate: /\b(generate|create|make|build|produce|draw|paint|render)\b/i,

      search: /\b(search|find|look for|show me|list|browse)\b/i,

      chain: /\b(chain|pipeline|workflow|multi-step|sequence)\b/i,

      explain: /\b(explain|what is|how does|tell me about|describe)\b/i,

      compare: /\b(compare|versus|vs|difference|better)\b/i,

      optimize: /\b(optimize|improve|enhance|best|fastest|cheapest)\b/i

    }

    def self.classify(input)
      INTENTS.each do |intent, pattern|

        return intent if input.match?(pattern)

      end

      :generate # Default intent

    end

    def self.extract_params(input, intent)
      params = {}

      case intent
      when :generate

        params[:prompt] = input.gsub(INTENTS[:generate], '').strip

        params[:chain_type] = detect_chain_type(input)

      when :search

        params[:query] = input.gsub(INTENTS[:search], '').strip

      when :chain

        params[:style] = extract_style(input)

        params[:prompt] = input.gsub(INTENTS[:chain], '').strip

      when :compare

        params[:models] = extract_model_names(input)

      end

      params
    end

    def self.detect_chain_type(input)
      return :video if input.match?(/\b(video|motion|animate|movie)\b/i)

      return :full if input.match?(/\b(full|complete|with music|with sound)\b/i)

      return :creative if input.match?(/\b(creative|artistic|experimental)\b/i)

      :quick

    end

    def self.extract_style(input)
      return "cinematic" if input.match?(/\b(cinematic|film|movie)\b/i)

      return "dramatic" if input.match?(/\b(dramatic|intense)\b/i)

      return "experimental" if input.match?(/\b(experimental|chaos|random)\b/i)

      "cinematic"

    end

    def self.extract_model_names(input)
      # Extract model IDs from input

      input.scan(/[\w-]+\/[\w-]+/).uniq

    end

  end

  class ConversationalAgent
    def initialize(vector_store, llm_router)

      @vector_store = vector_store

      @llm_router = llm_router

      @context = []

    end

    def process(user_input)
      # Classify intent

      intent = IntentClassifier.classify(user_input)

      params = IntentClassifier.extract_params(user_input, intent)

      # Add to conversation context
      @context << { role: "user", content: user_input }

      case intent
      when :search

        handle_search(params[:query])

      when :generate

        handle_generate(params)

      when :chain

        handle_chain(params)

      when :explain

        handle_explain(user_input)

      when :compare

        handle_compare(params[:models])

      when :optimize

        handle_optimize(user_input)

      else

        handle_fallback(user_input)

      end

    end

    def handle_search(query)
      results = @vector_store.semantic_search(query, limit: 10)

      {
        intent: :search,

        results: results,

        summary: format_search_results(results)

      }

    end

    def handle_generate(params)
      # Use LLM to suggest best generation approach

      prompt = build_generation_prompt(params[:prompt], params[:chain_type])

      response = @llm_router.query(prompt, max_tokens: 1000)

      parse_llm_recommendation(response) || {
        intent: :generate,

        chain_type: params[:chain_type],

        prompt: params[:prompt]

      }

    end

    def handle_chain(params)
      # Build optimal chain based on style and prompt

      relevant_models = @vector_store.semantic_search(params[:prompt], limit: 20)

      prompt = build_chain_prompt(params[:prompt], params[:style], relevant_models)
      response = @llm_router.query(prompt, max_tokens: 1500)

      {
        intent: :chain,

        style: params[:style],

        recommendation: parse_chain_response(response),

        models: relevant_models.first(5)

      }

    end

    def handle_explain(query)
      models = @vector_store.semantic_search(query, limit: 3)

      if models.any?
        model = models.first

        {

          intent: :explain,

          model: model,

          explanation: "#{model['id']}: #{model['description']}"

        }

      else

        {

          intent: :explain,

          message: "No models found for: #{query}"

        }

      end

    end

    def handle_compare(model_ids)
      {

        intent: :compare,

        models: model_ids,

        message: "Comparing: #{model_ids.join(' vs ')}"

      }

    end

    def handle_optimize(query)
      prompt = <<~PROMPT

        Optimize this AI generation request for cost and quality: "#{query}"

        Suggest:
        1. Most cost-effective model chain

        2. Quality vs speed tradeoffs

        3. Estimated costs

        Respond as JSON with: {approach, models, cost, reasoning}
      PROMPT

      response = @llm_router.query(prompt)
      {
        intent: :optimize,

        recommendation: parse_optimization(response)

      }

    end

    def handle_fallback(input)
      {

        intent: :unknown,

        message: "I can help with: generate, search, chain, explain, compare, optimize",

        suggestion: "Try: 'generate a sunset photo' or 'search for video models'"

      }

    end

    private
    def build_generation_prompt(user_prompt, chain_type)
      <<~PROMPT

        User wants to generate: "#{user_prompt}"

        Preferred chain type: #{chain_type}

        From these Replicate models, suggest the best approach:
        - imagen3 ($0.01): Fast image generation

        - flux ($0.03): High-quality photorealistic

        - wan480 ($0.08): Image to video

        - music ($0.02): Audio generation

        - upscale ($0.002): 4x upscaling

        Respond as JSON:
        {

          "approach": "single|chain",

          "models": [{"id": "model-name", "reasoning": "why"}],

          "estimated_cost": 0.05,

          "explanation": "brief summary"

        }

      PROMPT

    end

    def build_chain_prompt(user_prompt, style, models)
      models_summary = models.first(10).map do |m|

        "- #{m['id']} (#{m['type']}, $#{m['cost'] || 0.05})"

      end.join("\n")

      <<~PROMPT
        Build a #{style} generation chain for: "#{user_prompt}"

        Available models:
        #{models_summary}

        Design a 3-8 step pipeline. Respond as JSON:
        {

          "steps": [{"model": "id", "purpose": "what it does"}],

          "total_cost": 0.15,

          "explanation": "why this chain works"

        }

      PROMPT

    end

    def format_search_results(results)
      if results.empty?

        "No models found"

      else

        "Found #{results.size} models:\n" + results.first(5).map do |m|

          "â€¢ #{m['id']} (#{m['type']}) - #{m['description']&.slice(0, 60)}"

        end.join("\n")

      end

    end

    def parse_llm_recommendation(response)
      return nil unless response

      json_match = response.match(/\{[\s\S]*\}/)
      return nil unless json_match

      JSON.parse(json_match[0])
    rescue

      nil

    end

    def parse_chain_response(response)
      parse_llm_recommendation(response)

    end

    def parse_optimization(response)
      parse_llm_recommendation(response)

    end

  end

end

# Standalone CLI for testing
if __FILE__ == $0

  puts "Repligen NLU v#{RepligenNLU::VERSION}"

  puts "="*60

  # Initialize components
  vector_store = RepligenNLU::VectorStore.new

  llm_router = RepligenNLU::LLMRouter.new

  agent = RepligenNLU::ConversationalAgent.new(vector_store, llm_router)

  # Index models if needed
  if ARGV.include?("--index")

    vector_store.index_all_models

  end

  # Interactive mode
  puts "\nNLU Agent ready. Try:"

  puts "  'generate a sunset photo'"

  puts "  'search for video models'"

  puts "  'build a cinematic chain for portrait'"

  puts ""

  loop do
    print "> "

    input = gets&.chomp

    break if input.nil? || input.empty? || %w[quit exit q].include?(input.downcase)

    result = agent.process(input)
    puts "\n#{JSON.pretty_generate(result)}\n"

  end

  vector_store.close
end

